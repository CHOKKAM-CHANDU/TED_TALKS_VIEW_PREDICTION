{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **TED Talks Views Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member**           - Chandu Chokkam\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Summary: TED Talk View Prediction\n",
        "Objective:\n",
        "The goal of this project is to develop a predictive model to estimate the number of views a TED Talk will receive based on features extracted from the dataset, such as the talk's topic, duration, description, publication year, and speaker details.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "Problem Definition:\n",
        "\n",
        "Predict the number of views a TED Talk receives.\n",
        "Identify the key factors contributing to the popularity of a talk.\n",
        "\n",
        "Dataset:\n",
        "\n",
        "A publicly available TED Talks dataset containing attributes such as:\n",
        "Title: Name of the talk.\n",
        "Speaker name: The person delivering the talk.\n",
        "Duration: Length of the talk in seconds.\n",
        "Tags: Keywords associated with the talk.\n",
        "Number of comments: Engagement metric.\n",
        "Event: The event where the talk was given.\n",
        "Publication date: When the talk was made public.\n",
        "Views: Target variable for prediction.\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Analyze the distribution of views and other features.\n",
        "Examine correlations between views and other attributes.\n",
        "Visualize popular topics, durations, and other key trends.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Extract and process features such as:\n",
        "Text analysis of title, description, and tags (e.g., sentiment, word count).\n",
        "Time-based features (e.g., year, month of publication).\n",
        "Event-based popularity trends.\n",
        "Encode categorical variables and normalize numerical data.\n",
        "\n",
        "Model Development:\n",
        "\n",
        "Split data into training and testing sets.\n",
        "Use machine learning models such as:\n",
        "Linear Regression\n",
        "Random Forest\n",
        "Gradient Boosting (e.g., XGBoost, LightGBM)\n",
        "Neural Networks (optional for advanced users).\n",
        "Evaluate model performance using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), or RÂ² score.\n",
        "\n",
        "Insights and Recommendations:\n",
        "\n",
        "Identify key drivers of high viewership.\n",
        "Provide actionable insights for TED organizers and speakers to optimize talks for popularity.\n",
        "Deployment (Optional):\n",
        "\n",
        "Build a simple web-based interface to input talk details and predict expected views using the trained model.\n",
        "\n",
        "Potential Applications:\n",
        "\n",
        "Help speakers understand factors that drive TED Talk popularity.\n",
        "Assist event organizers in scheduling and promoting talks more effectively.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/CHOKKAM-CHANDU/TED_TALKS_VIEW_PREDICTION"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Datatime library for Date columns\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "# for remove Multicollinearity\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Preprocessing libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# For build pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# Machine learning models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor,StackingRegressor\n",
        "\n",
        "\n",
        "# for plot decision tree\n",
        "from sklearn import tree\n",
        "\n",
        "# Model selection libraries\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# importing XGB regressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Metrics libraries for model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Warnings module handles warnings in Python\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "d = pd.read_csv('/content/ted.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "d.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "d.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   talk_id: A unique identifier for each TED Talk video.\n",
        "*   title: The title of the talk.\n",
        "*   speaker_1: The primary speaker for the talk.\n",
        "\n",
        "*   all_speakers: A list of all the speakers for the talk.\n",
        "\n",
        "*   occupations: The occupations of the speakers.\n",
        "*   about_speakers: Information about the speakers, such as their backgrounds and expertise.\n",
        "*   recorded_date: The date the talk was recorded.\n",
        "\n",
        "\n",
        "*   published_date: The date the talk was published on the TED Talks YouTube channel.\n",
        "\n",
        "\n",
        "*   event: The name of the TED event where the talk was given.\n",
        "\n",
        "*   native_lang: The language the talk was given in.\n",
        "*   available_lang: The languages the talk is available in.\n",
        "\n",
        "*   duration: The length of the video.(in sec.)\n",
        "*   topics: The topics covered in the talk.\n",
        "\n",
        "*   related talks: Other TED Talks that are related to this talk.\n",
        "*   url: The URL of the video.\n",
        "\n",
        "*   description: A brief description of the talk.\n",
        "*   transcript: A transcript of the talk.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "d.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "d.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "d.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   comments , occupations and about_speakers columns are high missing values.\n",
        "*   The comments attributes has a lot of NaN values(655) to deal with. We have used some basic intution for what could be the reason of comments being null. The most logical explanation could be that the comments are disabled for the video. The other reason that could be possible is the data inconsistency so there could be some issues with the survey that are possibly causing these inconsistencies. We'll deal with these NaN values later on.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d36IIc8JTPcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "d['occupations'].duplicated().sum()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "d['about_speakers'].duplicated().sum()"
      ],
      "metadata": {
        "id": "h1TSrKhqI5xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "d[d['recorded_date'].isnull()]"
      ],
      "metadata": {
        "id": "TqnHWByuJPx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "d[d['all_speakers'].isnull()]"
      ],
      "metadata": {
        "id": "bHXHXLwNKEpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TED Talks dataset contains 4,005 entries with features like title, speaker_1, views, duration, topics, and description. Some fields have missing data, such as occupations (3483 non-null) and comments (3350 non-null). It includes dates (recorded_date, published_date), multilingual data (native_lang, available_lang), and engagement metrics like views and comments. Text fields like description and transcript can be leveraged for NLP tasks."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "d.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "d.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe the numerical dataset\n",
        "d.describe().T"
      ],
      "metadata": {
        "id": "TVhOm9SYUADb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.describe(percentiles=[.25,.50,.75,.80,.85,.90,.95,.96,.97,.98,.99])"
      ],
      "metadata": {
        "id": "bjhvsmjiUGEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   the minimum value of views is 0.\n",
        "\n",
        "*   the minimum value of comments is also 0.\n",
        "\n",
        "*   outliers in views, comments and duration columns.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwNX_8YEUQrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find rows where column comments have 0 value\n",
        "d[d['comments']==0.0]"
      ],
      "metadata": {
        "id": "UgusVESkU5WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find rows where column views have 0 value\n",
        "d[d['views']==0]"
      ],
      "metadata": {
        "id": "L0NgzYV7VAu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Total 6 rows are present where views = 0 and columns = NaN. this is MCAR data (missing completely at random) so we can remove this rows. because this is impossible that the views of video are 0 on TEDx Website.\n",
        "*   Total 655 NaN values present in comments column so we have to fill that value also.\n",
        "\n"
      ],
      "metadata": {
        "id": "2giIcVugVQMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in d.columns:\n",
        "  print(f'The unique values in {i} are {d[i].nunique()}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numerical Columns =** talk_id, views, comments, duration.\n",
        "\n",
        "**categorical columns =** title, speaker_1, all_speakers, occupations, about_speakers, event, native_lang, available_lang, topics,\n",
        "                      related_talks, url, description, transcript\n",
        "\n",
        "**Datetime columns =** recorded_date, published_date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   incorrect data-type assigned to recorded_date , published_date , comments.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NfFjQ2yeSkxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for 'native_lang' variable.\n",
        "d['native_lang'].unique()"
      ],
      "metadata": {
        "id": "hEBPeJBxLxJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "d.describe(include='O').T"
      ],
      "metadata": {
        "id": "2RibDt1QVd2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "d[(d['occupations'].isnull() & d['about_speakers'].isnull())][['speaker_1', 'all_speakers']]"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All Issues with the dataset\n",
        "\n",
        "1. Dirty Data (Low quality data)\n",
        "\n",
        "<> Comments , occupations and about_speakers columns are high missing values...we have to fill 655 missing values of comments. completion issue(missing data)\n",
        "\n",
        "<> Incorrect data-type assigned to recorded_date , published_date , comments.\n",
        "\n",
        "<> The minimum value of column views is 0 and there are total 6 rows. so we have to delete that rows.\n",
        "\n",
        "<> The minimum value of comments is also 0. there are only 2 rows there but null value in other 655 rows so simply fill with 0 but this column important so we fill values in feature engineering part. accuracy issue(not accurate values)\n",
        "\n",
        "<> There are two column with details, i.e. speaker_1, all_speakers. So, one of the column is to be deleted.\n",
        "\n",
        "<> url and talk_id column is also not useful in views prediction so, we have to delete both the columns.\n",
        "\n",
        "2. Messy Data (untidy data)\n",
        "\n",
        "<> Topics and available_lang are in list format. we have to split this untidy data for better feature corr with views. we perform this in feature transformation part.\n",
        "\n",
        "<> There are also few columns in dictionary untidy format occupations,about_speakers, related_talks but this columns are not important so in later feature transformation part we remove this columns if needed."
      ],
      "metadata": {
        "id": "BsCfKBlzSsW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copy of the original dataset\n",
        "df = d.copy()"
      ],
      "metadata": {
        "id": "MtvvcYDbVAHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling missing value.\n",
        "\n",
        "values = {'comments':0, 'occupations':'no data', 'about_speakers': 'no data', 'all_speakers' : 'no data'}\n",
        "\n",
        "df = df.fillna(value=values)"
      ],
      "metadata": {
        "id": "EFOOCfAxVrwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No null values now, I'll take care of views and comment zero values later in feature engineering part.\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "avEAEC1SWJWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the wrongly assigned data types\n",
        "df = df.astype({'talk_id': 'int32', 'views':'int32','comments':'int32', 'duration':'int32'})\n",
        "\n",
        "df['published_date'] = pd.to_datetime(df['published_date'])\n",
        "\n",
        "df['recorded_date'] = pd.to_datetime(df['recorded_date'])"
      ],
      "metadata": {
        "id": "QjaJnjp9W8JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the unnecessary columns & renaming the speaker1 column to speaker column\n",
        "\n",
        "df.drop(['talk_id', 'all_speakers', 'url'], axis = 1, inplace=True)\n",
        "\n",
        "df.rename(columns={'speaker_1':'speaker'}, inplace=True)"
      ],
      "metadata": {
        "id": "huXwg_dlYnZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where 'views' is 0.\n",
        "# Now df is a new dataframe which does not contain views column with zero value\n",
        "df = df[df['views'] != 0]"
      ],
      "metadata": {
        "id": "N5m8AKkeaUbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Checking the above operation\n",
        "print((df['views'] == 0).sum())"
      ],
      "metadata": {
        "id": "MjREfz7mbmOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing few more columns which are not important\n",
        "df.drop(['occupations', 'about_speakers', 'related_talks','description','transcript'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Ab12eBzqddh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking new shape of dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "g_D2kcdJbwOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking random samples\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "6g6lb8mRZ88b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find popular talk show titles and speakers based on views\n",
        "\n",
        "popular_talks = df[['title', 'speaker', 'views']].sort_values('views', ascending=False)[0:15]\n",
        "popular_talks"
      ],
      "metadata": {
        "id": "BqGjp7SDeGXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations :**\n",
        "\n",
        "*   Ken Robinson's talk on Do Schools Kill Creativity? is the most popular TED Talk of all time with 65.05 million views.\n",
        "*   Also coincidentally, it is also one of the first talks to ever be uploaded on the TED Site (the main dataset is sorted by published date).\n",
        "*   Robinson's talk is closely followed by Amy Cuddy's talk on Your Body Language May Shape Who You Are.\n",
        "*   There are only 3 talks that have surpassed the 50 million mark and 12 talks that have crossed the 30 million mark.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkP5Kc3hiPfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe with top 15 speakers by views\n",
        "top15_views = df.groupby('speaker').views.sum().nlargest(15)\n",
        "top15_views = top15_views.reset_index()\n",
        "top15_views\n"
      ],
      "metadata": {
        "id": "nnRoMJaUBcgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe with top 15 speakers by comments\n",
        "top15_comments = df.groupby('speaker').comments.sum().nlargest(15)\n",
        "top15_comments = top15_comments.reset_index()\n",
        "top15_comments"
      ],
      "metadata": {
        "id": "dt4UVwiGNzyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was first copied to preserve the original data, and missing values were handled by filling `comments` with `0`, and `occupations`, `about_speakers`, and `all_speakers` with `'no data'`. Data types were corrected by converting `talk_id`, `views`, `comments`, and `duration` to `int32`, while `published_date` and `recorded_date` were converted to `datetime`. Unnecessary columns, including `talk_id`, `all_speakers`, `url`, `occupations`, `about_speakers`, `related_talks`, `description`, and `transcript`, were dropped to streamline the dataset. The `speaker_1` column was renamed to `speaker` for clarity. Rows with `views` equal to `0` were removed, and a validation check confirmed that no such rows remained. These manipulations ensured a cleaner and more consistent dataset for further analysis and feature engineering."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# create the figure and subplots\n",
        "fig, axs = plt.subplots(2,1, figsize=(18,12))\n",
        "\n",
        "# create a barplot with top 15 speakers by views\n",
        "sns.barplot(x='views', y='speaker', data=top15_views, ax=axs[0])\n",
        "axs[0].set_title('Top 15 Speakers by Views')\n",
        "\n",
        "# create a barplot with top 15 speakers by comments\n",
        "sns.barplot(x='comments', y='speaker', data=top15_comments, ax=axs[1])\n",
        "axs[1].set_title('Top 15 Speakers by Comments')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions and Answers:\n",
        "\n",
        "1. **Why did you pick the specific chart?**  \n",
        "   The bar plots were chosen because they effectively showcase rankings and comparisons, making it easy to identify the most viewed and commented speakers at a glance.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**  \n",
        "   Popular speakers like Alex Gendler and Sir Ken Robinson have high views, while Richard Dawkins and Sir Ken Robinson lead in comments. Themes like education, inspiration, and controversy tend to drive audience engagement.\n",
        "\n",
        "3. **Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth?**  \n",
        "   Yes, the insights can guide future content strategies by identifying trending themes and speakers, boosting engagement and revenue. However, over-reliance on popular themes may stifle innovation and lead to audience fatigue. Balancing popular and unique content is essential."
      ],
      "metadata": {
        "id": "I1qW6MNtC4IR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#checking corr. with views column\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='comments', y='views', data=df)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q&A Based on the Chart\n",
        "\n",
        "1. **Why did you pick the specific chart?**  \n",
        "   This scatter plot effectively shows the relationship between the number of views and comments for TED Talks. It allows us to identify patterns, such as whether higher views lead to higher comments or if there are any outliers.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**  \n",
        "   - The majority of TED Talks cluster around low views and comments, indicating moderate engagement levels.  \n",
        "   - Outliers with exceptionally high views or comments highlight specific talks that resonate significantly with the audience.\n",
        "\n",
        "3. **Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with a specific reason.**  \n",
        "   - **Positive Impact**: Understanding which talks generate high engagement helps TED focus on popular themes or speakers to maximize audience interaction.  \n",
        "   - **Negative Impact**: Over-prioritizing popular themes might reduce diversity and innovation, as niche or emerging topics with lower engagement could be overlooked."
      ],
      "metadata": {
        "id": "evob05xNDypY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# checking distribution of comments column\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['comments'], color='Red')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q&A Based on the Chart\n",
        "\n",
        "1. **Why did you pick the specific chart?**  \n",
        "   A density plot with a histogram provides a clear visualization of the distribution of comments. It shows the frequency and concentration of comments for TED Talks, making it easy to observe where most talks lie on the spectrum of engagement.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**  \n",
        "   - The majority of TED Talks receive fewer than 500 comments, with a sharp decline as the number of comments increases.  \n",
        "   - A long tail exists for talks with higher comment counts, indicating a few talks generate significantly more discussion than the rest.\n",
        "\n",
        "3. **Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with a specific reason.**  \n",
        "   - **Positive Impact**: Identifying the characteristics of highly commented talks can help TED optimize future content for engagement.  \n",
        "   - **Negative Impact**: Overfocusing on the majority of low-comment talks might lead to missing opportunities to create more impactful and engaging content."
      ],
      "metadata": {
        "id": "oeIbAC1hD5kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df[df['comments'] > 1100]))"
      ],
      "metadata": {
        "id": "BzDZPdRFDWvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping indexes where comments are greater than 1100\n",
        "df.drop(df[df['comments']>1100].index, inplace=True)"
      ],
      "metadata": {
        "id": "fBMsonOvDFDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill null(0) value with median of column\n",
        "\n",
        "df['comments']= df['comments'].replace(0, np.nan)\n",
        "df[\"comments\"].fillna(df[\"comments\"].median(), axis = 0, inplace = True)"
      ],
      "metadata": {
        "id": "1SoFt9OBDcvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking distribution of comments column\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['comments'], color='Red')"
      ],
      "metadata": {
        "id": "m5r0TgYNDcr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziudZvuHDcg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHART-4"
      ],
      "metadata": {
        "id": "7iClYuMNDGzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# check distribution of views column\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['views'], color ='green')\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q&A Based on the Chart\n",
        "\n",
        "1. **Why did you pick the specific chart?**  \n",
        "   A density plot with a histogram was chosen to show the distribution of views for TED Talks. This visualization effectively highlights how viewership is concentrated and the range of outliers.\n",
        "\n",
        "2. **What is/are the insight(s) found from the chart?**  \n",
        "   - Most TED Talks receive a relatively low number of views, with the majority concentrated under 10 million.  \n",
        "   - A long tail exists for videos with exceptionally high views, indicating a few highly successful talks dominate the overall viewership.\n",
        "\n",
        "3. **Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with a specific reason.**  \n",
        "   - **Positive Impact**: Insights can guide TED to analyze the common traits of highly viewed talks (e.g., topics, speakers) and replicate their success.  \n",
        "   - **Negative Impact**: Over-reliance on creating content similar to highly viewed talks might reduce diversity and innovation in the topics covered."
      ],
      "metadata": {
        "id": "Z0YmngmHEBGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHART-5\n"
      ],
      "metadata": {
        "id": "vvlvLs0rDJ1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# check distribution of duration column\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['duration'], color ='Orange')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why this chart?\n",
        "Examining duration distribution ensures optimal video length for maximum engagement.\n",
        "\n",
        "Insights:\n",
        "\n",
        "TED Talks have varying durations, with a possible preference for mid-length videos.\n",
        "Extreme durations may not be optimal for engagement.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Positive: Tailoring video lengths based on audience preferences.\n",
        "Negative: Over-standardizing lengths could limit creative freedom"
      ],
      "metadata": {
        "id": "Z3YbAC-eEIfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change duration in sec. to min.\n",
        "\n",
        "df['duration'] = df['duration'] / 60"
      ],
      "metadata": {
        "id": "9p9wwAvBDouJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'speaker_popularity' in the main DataFrame and assign the categories\n",
        "\n",
        "df['speaker_popularity'] = \"\"\n",
        "df.loc[df['views'] <= 500000, 'speaker_popularity'] = 'not_popular'\n",
        "df.loc[(df['views'] > 500000) & (df['views'] <= 1500000), 'speaker_popularity'] = 'avg_popular'\n",
        "df.loc[(df['views'] > 1500000) & (df['views'] <= 2500000), 'speaker_popularity'] = 'popular'\n",
        "df.loc[(df['views'] > 2500000) & (df['views'] <= 3500000), 'speaker_popularity'] = 'high_popular'\n",
        "df.loc[df['views'] > 3500000, 'speaker_popularity'] = 'extreme_popular'\n",
        "\n",
        "# check the dataset\n",
        "\n",
        "df.sample(2)"
      ],
      "metadata": {
        "id": "xmDgmP4mDpiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.barplot(data=df, x='speaker_popularity', y='comments',\n",
        "            order=['not_popular', 'avg_popular', 'popular', 'high_popular', 'extreme_popular'])"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you pick the specific chart?\n",
        "A density plot with a histogram provides a clear visualization of the distribution of comments. It shows the frequency and concentration of comments for TED Talks, making it easy to observe where most talks lie on the spectrum of engagement.\n",
        "\n",
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "The majority of TED Talks receive fewer than 500 comments, with a sharp decline as the number of comments increases.\n",
        "A long tail exists for talks with higher comment counts, indicating a few talks generate significantly more discussion than the rest.\n",
        "Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with a specific reason.\n",
        "\n",
        "Positive Impact: Identifying the characteristics of highly commented talks can help TED optimize future content for engagement.\n",
        "Negative Impact: Overfocusing on the majority of low-comment talks might lead to missing opportunities to create more impactful and engaging content.\n"
      ],
      "metadata": {
        "id": "Pa1XKDUlELvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'video_rating' in the main DataFrame and assign the categories\n",
        "\n",
        "df['video_rating'] = \"\"\n",
        "df.loc[df['comments'] <= 50, 'video_rating'] = 1\n",
        "df.loc[(df['comments'] > 50) & (df['comments'] <= 120), 'video_rating'] = 2\n",
        "df.loc[(df['comments'] > 120) & (df['comments'] <= 200), 'video_rating'] = 3\n",
        "df.loc[(df['comments'] > 200) & (df['comments'] <= 300), 'video_rating'] = 4\n",
        "df.loc[df['comments'] > 300, 'video_rating'] = 5\n",
        "\n",
        "# check the dataset\n",
        "df.sample(2)"
      ],
      "metadata": {
        "id": "Xoz3wHZoGh-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add new column available_languages using existing column available_lang\n",
        "\n",
        "df['available_languages'] = df['available_lang'].apply(lambda x: len(x))\n",
        "pd.DataFrame(df['available_languages'])"
      ],
      "metadata": {
        "id": "z7Qs3CAAHc9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# check the distribution of this new column available_languages\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(df['available_languages'],color = 'darkblue')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you pick the specific chart?\n",
        "A density plot with a histogram was chosen to visualize the distribution of available languages for TED Talks because it effectively highlights the frequency and spread of data. This combination helps understand how multilingual accessibility is distributed across the dataset.\n",
        "\n",
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "The distribution of available languages is approximately normal, with most TED Talks offering translations in around 200 languages.\n",
        "Few talks are available in fewer than 100 or more than 300 languages, showing a central focus on talks with translations in a moderate range.\n",
        "The long tail suggests that only a small fraction of talks deviate significantly in either direction.\n",
        "Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with a specific reason.\n",
        "\n",
        "Positive Impact: Understanding the optimal range of available languages (around 200) can help TED prioritize resources for translation services to maximize accessibility.\n",
        "Negative Impact: Overextending translation efforts for less popular talks might lead to wasted resources without significantly improving audience engagement.\n"
      ],
      "metadata": {
        "id": "OYUXYITsEQ8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_columns = d.select_dtypes(include=['number'])\n",
        "\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "\n",
        "# Add title and display the heatmap\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Knt9AlU5HDSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you pick the specific chart?\n",
        "\n",
        "A correlation heatmap is a powerful visualization to understand the relationships between numerical variables in a dataset. It provides an easy-to-read summary of how strongly features are correlated with each other, aiding in feature selection and understanding dependencies.\n",
        "\n",
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "Positive Correlation: There is a moderate positive correlation of 0.50 between views and comments, suggesting that talks with higher views tend to have more comments.\n",
        "\n",
        "Negative Correlation: There is a weak negative correlation between duration and talk_id (-0.26), indicating minimal dependency between the two variables.\n",
        "Low Correlation: Features like duration and views (0.07) have almost no correlation, which means they are largely independent of each other.\n",
        "\n",
        "Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with a specific reason.\n",
        "\n",
        "Positive Impact: The moderate positive correlation between views and comments suggests that engaging and widely viewed talks tend to spark more conversations. By focusing on promoting content that encourages interaction, TED can optimize user engagement.\n",
        "\n",
        "Negative Impact: Misinterpreting weak correlations (e.g., duration and views) could lead to ineffective efforts in designing talks based on length, which may not drive significant growth. It's essential to rely on strong correlations when making strategic decisions."
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(d, diag_kind=\"kde\", plot_kws={\"alpha\": 0.6})\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why did you pick the specific chart?  \n",
        "The pair plot effectively visualizes pairwise relationships and distributions across numerical features, making it easy to detect correlations, trends, and outliers in the dataset.\n",
        "\n",
        "### What is/are the insight(s) found from the chart?  \n",
        "1. `talk_id` has no meaningful relationship with other features as it's an identifier.  \n",
        "2. A positive relationship exists between `views` and `comments`; talks with higher views tend to receive more comments.  \n",
        "3. Longer durations cluster around moderate views and comments but show no strong correlation.  \n",
        "4. Distributions of `views` and `comments` are highly skewed, with significant outliers.\n",
        "\n",
        "### Will the gained insights help create a positive business impact?  \n",
        "**Positive Impact**:  \n",
        "1. Insights on `views` and `comments` can help TED optimize content for engagement.  \n",
        "2. Understanding `duration` trends enables tailoring talk lengths for better audience retention.  \n",
        "**Negative Impact**:  \n",
        "1. Overemphasis on popular talks might ignore niche audiences.  \n",
        "2. Unhandled outliers could bias future content strategies.  "
      ],
      "metadata": {
        "id": "vHzT18UWIF5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE ENGINEERING**"
      ],
      "metadata": {
        "id": "khnHw8S7IYMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making seperate column for day, month and year of upload\n",
        "\n",
        "df['published_year'] = df['published_date'].dt.year\n",
        "df['published_month'] = df['published_date'].dt.month\n",
        "df['published_day'] = df[\"published_date\"].dt.day_name()\n",
        "\n",
        "# storing weekdays in order of numbers from 0 to 6 value\n",
        "\n",
        "daydict = {'Sunday' : 0, 'Monday' : 1, 'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6}\n",
        "\n",
        "# making new column holding information of day number\n",
        "\n",
        "df['published_daynumber'] = df['published_day'].map(daydict)\n"
      ],
      "metadata": {
        "id": "9JPt8IpvIH5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add one more column published_months_ago\n",
        "\n",
        "df['published_months_ago'] = ((2023 - df['published_year'])*12 + df['published_month'])\n"
      ],
      "metadata": {
        "id": "CsUA9TmlIWW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(1)"
      ],
      "metadata": {
        "id": "6awHo7Z7Iddd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there are lot of TED events\n",
        "\n",
        "print(df['event'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "vKKNw7kOJFMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add new column of each TED event type using existing column event\n",
        "\n",
        "ted_categories = ['TED-Ed','TEDx', 'TED', 'TEDGlobal', 'TEDSummit', 'TEDWomen', 'TED Residency']\n",
        "\n",
        "\n",
        "df['TEDevent_type'] = df['event'].map(lambda x: \"TEDx\" if x[0:4] == \"TEDx\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TED-Ed\" if x[0:4] == \"TED_Ed\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TED\" if x[0:4] == \"TED2\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TEDGlobal\" if x[0:4] == \"TEDG\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TEDWomen\" if x[0:4] == \"TEDW\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TEDSummit\" if x[0:4] == \"TEDS\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"TED Residency\" if x[0:13] == \"TED Residency\" else x)\n",
        "df['TEDevent_type'] = df['TEDevent_type'].map(lambda x: \"Other TED\" if x not in ted_categories else x)"
      ],
      "metadata": {
        "id": "WpacaVWvJEA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# check the all events talkshows counts\n",
        "\n",
        "pd.DataFrame(df['TEDevent_type'].value_counts()).reset_index()"
      ],
      "metadata": {
        "id": "YqYgscfFJYnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# use duplicate dataframe for topics analysis\n",
        "dff = df.copy()\n",
        "\n",
        "dff['topics'] = dff['topics'].apply(lambda x: ast.literal_eval(x))\n",
        "s = dff.apply(lambda x: pd.Series(x['topics']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'topic'\n",
        "\n",
        "dff = dff.drop('topics', axis=1).join(s)"
      ],
      "metadata": {
        "id": "RpRjpAnCUcdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a bar chart of popular topics of TEDx Website\n",
        "\n",
        "pop_topic = pd.DataFrame(dff['topic'].value_counts()).reset_index()\n",
        "pop_topic.columns = ['topic', 'TEDtalks']\n",
        "\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(x='topic', y='TEDtalks', data=pop_topic.head(12))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BN54iOgdUtRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(labels = [\"speaker\", \"title\", \"recorded_date\", \"published_date\", \"event\", \"native_lang\", \"available_lang\", \"topics\"],axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "3PT68LKsVTZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again change data-types of columns\n",
        "\n",
        "df = df.astype({'comments':'int64', 'views':'int64','video_rating':'int64'})\n",
        "\n",
        "df = df.astype({\n",
        "    'speaker_popularity': 'category',\n",
        "    'published_day': 'category',\n",
        "    'TEDevent_type': 'category'\n",
        "})"
      ],
      "metadata": {
        "id": "Cm0ug7z5Vj__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multicollinearity"
      ],
      "metadata": {
        "id": "d5O9MGRkVzZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# create a new DataFrame with only numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'int32', 'float32', 'float64']).drop(['views'], axis=1)\n",
        "\n",
        "# calculate VIF for each column\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(numeric_cols.values, i) for i in range(numeric_cols.shape[1])]\n",
        "vif[\"features\"] = numeric_cols.columns\n",
        "\n",
        "# print the results\n",
        "vif"
      ],
      "metadata": {
        "id": "GezwMQxKV2b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns published_year and published_months_ago are highly correlated with each other and have high VIF. We can remove one of these columns and check VIF again."
      ],
      "metadata": {
        "id": "j9guhXrJWFdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: drop published_months_ago column\n",
        "\n",
        "df.drop(['published_year','published_month', 'published_months_ago','video_rating'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Step 2: calculate VIF\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'int32', 'float32', 'float64']).drop(['views'], axis=1)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(numeric_cols.values, i) for i in range(numeric_cols.shape[1])]\n",
        "vif[\"features\"] = numeric_cols.columns\n",
        "\n",
        "\n",
        "# print the results\n",
        "\n",
        "vif\n"
      ],
      "metadata": {
        "id": "__Nw63I1WGDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use Yeo - Johnson Transform for views column and then we train test split the data\n",
        "\n",
        "pt = PowerTransformer()\n",
        "df['views'] = pt.fit_transform(pd.DataFrame(df['views']))"
      ],
      "metadata": {
        "id": "fq4nGQm1WYUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# split the Dataset into independent(x) and dependent(y) Dataset\n",
        "\n",
        "X = df.drop(columns=['views'])\n",
        "y = df['views']\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display independent variables dataframe\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "ZgwQ00lcXXc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display dependent variable dataframe\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "TLIsKgE8XbF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ltBf2NwQXjqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling train_test_split() to get the training and testing data.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "\n",
        "# split sizes\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "id": "OKgUo0TsYaF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# using column transformer to make step1 of scaling, encoding, function transformer, imputer etc to build pipelines.\n",
        "step1 = ColumnTransformer(transformers=[\n",
        "    ('col_tnf', StandardScaler(),[0,1,3,5]),\n",
        "    ('col_tnf1', PowerTransformer(),[0,1,3]),\n",
        "    ('col_tnf2', OneHotEncoder(sparse_output=False, drop='first'),[4,6]), # Change 'sparse' to 'sparse_output'\n",
        "    ('col_tnf3', OrdinalEncoder(categories=[['not_popular','avg_popular','popular','high_popular','extreme_popular']]),[2])\n",
        "],remainder='passthrough')\n",
        "\n",
        "\n",
        "\n",
        "# display pipeline\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nbk0q4iflshH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By utilizing a ColumnTransformer, we can efficiently apply multiple pre-processing steps, such as scaling, encoding and function transformation, to our data in a single step. This simplifies the pre-processing phase and allows us to build pipelines with different algorithms, performing hyperparameter tuning to find the best results for our model."
      ],
      "metadata": {
        "id": "GyT2nFTIZIS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply LinearRegression algorithm as step2\n",
        "\n",
        "step2 = LinearRegression()\n",
        "\n",
        "\n",
        "# make pipeline\n",
        "pipe1 = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "# fit the pipeline on training dataset\n",
        "pipe1.fit(X_train,y_train)\n",
        "\n",
        "# predict the train and test dataset\n",
        "y_pred_train = pipe1.predict(X_train)\n",
        "y_pred = pipe1.predict(X_test)\n",
        "\n",
        "# display pipeline diagram\n",
        "display(pipe1)\n",
        "# LinearRegression model all output scores\n",
        "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_train,y_pred_train))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_train,y_pred_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_test,y_pred))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mThe performance metrics\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('MAE',mean_absolute_error(y_test,y_pred))\n",
        "print('MSE',mean_squared_error(y_test,y_pred))\n",
        "print('RMSE',np.sqrt(mean_squared_error(y_test,y_pred)))\n"
      ],
      "metadata": {
        "id": "61ben_dWZdSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot the figure\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(y_pred)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No. of Test Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PZy14z5OZoO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# apply RidgeRegression algorithm with hyperparameter tuning as step2\n",
        "\n",
        "\n",
        "# giving parameters\n",
        "parameters = {'alpha': [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1,3,5,8,12,15,18,21,25]}\n",
        "\n",
        "# we use gridsearchCV because the dataset is not that big so we use this not RandomizedSearchCV\n",
        "Reg_ridge = GridSearchCV(Ridge(), parameters, cv=10)\n",
        "\n",
        "step2 = Reg_ridge\n",
        "\n",
        "# make pipeline\n",
        "pipe2 = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "# fit the pipeline on training dataset\n",
        "pipe2.fit(X_train,y_train)\n",
        "\n",
        "# predict the train and test dataset\n",
        "y_pred_train = pipe2.predict(X_train)\n",
        "y_pred = pipe2.predict(X_test)\n",
        "\n",
        "# display pipeline diagram\n",
        "display(pipe2)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot the figure\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(y_pred)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No. of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W6K2EmWeZ9BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# apply LassoRegression algorithm with hyperparameter tuning as step2\n",
        "\n",
        "\n",
        "# giving parameters\n",
        "parameters = {'alpha': [1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1,2,3,4,5,8,12,15,18,21,25]}\n",
        "\n",
        "# we use gridsearchCV because the dataset is not that big so we use this not RandomizedSearchCV\n",
        "Reg_Lasso = GridSearchCV(Lasso(), parameters, cv=10)\n",
        "\n",
        "step2 = Reg_Lasso\n",
        "\n",
        "# make pipeline\n",
        "pipe3 = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "# fit the pipeline on training dataset\n",
        "pipe3.fit(X_train,y_train)\n",
        "\n",
        "# predict the train and test dataset\n",
        "y_pred_train = pipe3.predict(X_train)\n",
        "y_pred = pipe3.predict(X_test)\n",
        "\n",
        "# display pipeline diagram\n",
        "display(pipe3)\n",
        "\n",
        "# Lasso Regression model all output scores\n",
        "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_train,y_pred_train))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_train,y_pred_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_test,y_pred))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mCross-validation score and best params\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print(\"The best parameters is\", Reg_Lasso.best_params_)\n",
        "print('cross-validation score', Reg_Lasso.best_score_)\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mThe performance metrics\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('MAE',mean_absolute_error(y_test,y_pred))\n",
        "print('MSE',mean_squared_error(y_test,y_pred))\n",
        "print('RMSE',np.sqrt(mean_squared_error(y_test,y_pred)))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot the figure\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(y_pred)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No. of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aVCyAdxMafmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 4"
      ],
      "metadata": {
        "id": "9O6sfKsRaofb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply DecisionTreeRegressor algorithm with hyperparameter tuning as step2\n",
        "\n",
        "\n",
        "# giving parameters\n",
        "parameters = {\n",
        "    'criterion':['squared_error'],     # 'friedman_mse', 'absolute_error'\n",
        "    'splitter' :['best'],              # random\n",
        "    'max_depth' :[6],                  #4,5,6,7,8,9,None\n",
        "    'max_features' :[1.0]              #0.25,0.50,0.75,0.85\n",
        "}\n",
        "\n",
        "# we use gridsearchCV because the dataset is not that big so we use this not RandomizedSearchCV\n",
        "dtr = GridSearchCV(DecisionTreeRegressor(), param_grid=parameters , cv=10, n_jobs=-1)\n",
        "\n",
        "step2 = dtr\n",
        "\n",
        "# make pipeline\n",
        "pipe4 = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "\n",
        "# fit the pipeline on training dataset\n",
        "pipe4.fit(X_train,y_train)\n",
        "# predict the train and test dataset\n",
        "y_pred_train = pipe4.predict(X_train)\n",
        "y_pred = pipe4.predict(X_test)\n",
        "\n",
        "# display pipeline diagram\n",
        "display(pipe4)\n",
        "\n",
        "# DecisionTreeRegressor model all output scores\n",
        "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_train,y_pred_train))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_train,y_pred_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_test,y_pred))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))))\n",
        "print('\\n')\n",
        "print('\\033[1mCross-validation score and best params\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print(\"The best parameters is\", dtr.best_params_)\n",
        "print('cross-validation score', dtr.best_score_)\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mThe performance metrics\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('MAE',mean_absolute_error(y_test,y_pred))\n",
        "print('MSE',mean_squared_error(y_test,y_pred))\n",
        "print('RMSE',np.sqrt(mean_squared_error(y_test,y_pred)))"
      ],
      "metadata": {
        "id": "xMrEo00Oan6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot the figure\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(y_pred)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No. of Test Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "spGp_UGKbCcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 5"
      ],
      "metadata": {
        "id": "O2K2v15XjPAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply RandomForestRegressor algorithm with hyperparameter tuning as step2\n",
        "\n",
        "\n",
        "# giving parameters\n",
        "parameters = {\n",
        "    'n_estimators':[58],      # 50,55,60,70,80,90,100\n",
        "    'max_depth' :[6],         # 4,5,6,7,8,9,None\n",
        "    'max_features' :[None],   # 'sqrt','log2'\n",
        "    'max_samples' :[0.85]     # 0.40,0.50,0.60,0.70,0.75,0.85,1.0\n",
        "}\n",
        "\n",
        "# we use gridsearchCV because the dataset is not that big so we use this not RandomizedSearchCV\n",
        "rfr = GridSearchCV(RandomForestRegressor(), param_grid=parameters , cv=10, n_jobs=-1)\n",
        "\n",
        "step2 = rfr\n",
        "\n",
        "# make pipeline\n",
        "pipe5 = Pipeline([\n",
        "    ('step1',step1),\n",
        "    ('step2',step2)\n",
        "])\n",
        "# fit the pipeline on training dataset\n",
        "pipe5.fit(X_train,y_train)\n",
        "\n",
        "# predict the train and test dataset\n",
        "y_pred_train = pipe5.predict(X_train)\n",
        "y_pred = pipe5.predict(X_test)\n",
        "\n",
        "# display pipeline diagram\n",
        "display(pipe5)\n",
        "\n",
        "# RandomForestRegressor model all output scores\n",
        "print('\\033[1mTraining data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_train,y_pred_train))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_train,y_pred_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mTesting data R2 and Adjusted R2 Score\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('R2 score',r2_score(y_test,y_pred))\n",
        "print('Adjusted R2 score', (1-(1-r2_score(y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))))\n",
        "\n",
        "print('\\n')\n",
        "print('\\033[1mCross-validation score and best params\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print(\"The best parameters is\", rfr.best_params_)\n",
        "print('cross-validation score', rfr.best_score_)\n",
        "print('\\n')\n",
        "print('\\033[1mThe performance metrics\\033[0m')\n",
        "print('\\033[1m' + '-----------------------------------------' + '\\033[0m')\n",
        "print('MAE',mean_absolute_error(y_test,y_pred))\n",
        "print('MSE',mean_squared_error(y_test,y_pred))\n",
        "print('RMSE',np.sqrt(mean_squared_error(y_test,y_pred)))"
      ],
      "metadata": {
        "id": "6lrNFVxCjORU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot the figure\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(y_pred)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No. of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3V8O8BiWjcof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating multiple regression models on the dataset, Random Forest Regressor perform better than other models. They have higher R2 scores, lower error metrics, and can generalize well on unseen data.\n",
        "\n",
        "Linear Regressor and Lasso Regressor have slightly lower performance metrics compared to Random Forest Regressor and Gradient Boosting Regressor.\n",
        "\n",
        "Decision Tree Regressor has a lower R2 score, higher error metrics, and little bit overfits the data comparing to the other best models, indicating it's not the best model to use.\n",
        "Therefore, based on the evaluation results, the Random Forest Regressor was chosen as the best model to achieve our objective. Also in future we can try implementing some other optimising techniques to wind up with better results.\n",
        "\n",
        "ð¥RandomForest with hyperparameter tuningð¥\n",
        "Training data R2 and Adjusted R2 Score\n",
        "\n",
        "R2 score 0.9108\n",
        "Adjusted R2 score 0.9106\n",
        "Testing data R2 and Adjusted R2 Score\n",
        "\n",
        "R2 score 0.8977\n",
        "Adjusted R2 score 0.\n",
        "\n",
        "Cross-validation score\n",
        "0.8974\n",
        "\n",
        "The performance metrics\n",
        "\n",
        "MAE 0.2613\n",
        "\n",
        "MSE 0.1055\n",
        "\n",
        "RMSE 0.3249\n",
        "\n",
        "At the end a word of Thankyou to you for going through project till the very end, genuinely appreciate your time. Happy Learning!"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}